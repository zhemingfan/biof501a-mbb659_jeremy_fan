Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	retrieve_reads
	1

[Sun Nov 29 10:11:45 2020]
rule retrieve_reads:
    output: data/SRR11801823_combined.fastq
    jobid: 0

[Sun Nov 29 10:12:17 2020]
Error in rule retrieve_reads:
    jobid: 0
    output: data/SRR11801823_combined.fastq
    shell:
        
        fasterq-dump --split-files SRR11801823 -O data | cat data/SRR11801823_1.fastq data/SRR11801823_2.fastq > data/SRR11801823_combined.fastq
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Removing output files of failed job retrieve_reads since they might be corrupted:
data/SRR11801823_combined.fastq
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /Users/owner/Documents/UBC/BIOF501A/term_project/biof501a-mbb659_jeremy_fan/.snakemake/log/2020-11-29T101145.643624.snakemake.log
